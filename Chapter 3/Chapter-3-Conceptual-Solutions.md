# Chapter-3-Conceptual-Solutions

## Questions And Answers
Exercise 3.2: Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions?

The MDP framework assumes the Markov property, namely that the state includes information about all aspects of the past agent-environment interaction that make a difference for the future.  An MDP framework is not really appropriate for a goal-directed learning task for which this Markov property is not true (e.g., where path dependency in arriving at a state also makes a difference for the future).

Another situation that might be difficult for an MDP framework would be if the goal-directed learning task had multiple goals it was trying to accomplish.  In this case, instead of the reward being being a single scalar, the reward would be a multi-component vector; in this case, the reinforcement learning scheme would probably have to invoke some additional constraint in order to perform the optimization.

Exercise 3.6: Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for -1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?

As discussed in the text, in the discounted, continuing task formulation, the return at each time would be related to ![$-\gamma^K$](https://render.githubusercontent.com/render/math?math=%24-%5Cgamma%5EK%24), where ![$K$](https://render.githubusercontent.com/render/math?math=%24K%24) is the number of time steps before failure.  In the discounted, episodic task formulation, the return at each time is still related to ![$-\gamma^K$](https://render.githubusercontent.com/render/math?math=%24-%5Cgamma%5EK%24), but ![$K$](https://render.githubusercontent.com/render/math?math=%24K%24) is constrained to be less than or equal to the total number of time steps of an episode ![$T$](https://render.githubusercontent.com/render/math?math=%24T%24).  Thus, the main difference is that the episodic task has a strict cap on the maximum return (which is related to ![$-\gamma^T$](https://render.githubusercontent.com/render/math?math=%24-%5Cgamma%5ET%24)).

Exercise 3.7: Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes--the successive runs through the maze--so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?

Treating the task as episodic with a reward of +1 for escaping the maze and a reward of 0 at all other times means that the robot experiences no penalty for exploring the maze without escape. As a result, the robot may learn to explore the maze until the number of time steps in an episode elapses.  An alternative treatment which may improve performance is to treat the task as episodic but to penalize the robot (e.g., prescribing a negative reward) for each time step spent in the maze without escape.

Exercise 3.8: Suppose ![$\gamma = 0.5$](https://render.githubusercontent.com/render/math?math=%24%5Cgamma%20%3D%200.5%24) and the following sequence of rewards is received: ![$R_1 = -1, R_2 = 2, R_3 = 6, R_4 = 3, R_5 = 2$](https://render.githubusercontent.com/render/math?math=%24R_1%20%3D%20-1%2C%20R_2%20%3D%202%2C%20R_3%20%3D%206%2C%20R_4%20%3D%203%2C%20R_5%20%3D%202%24), with ![$T = 5$](https://render.githubusercontent.com/render/math?math=%24T%20%3D%205%24). What are ![$G_0, G_1, \cdots, G_5$](https://render.githubusercontent.com/render/math?math=%24G_0%2C%20G_1%2C%20%5Ccdots%2C%20G_5%24)? Hint: Work backwards.

Using Equation 3.9 from the text (![$G_t = R_{t+1} + \gamma G_{t+1}$](https://render.githubusercontent.com/render/math?math=%24G_t%20%3D%20R_%7Bt%2B1%7D%20%2B%20%5Cgamma%20G_%7Bt%2B1%7D%24)) with ![$G_T = G_5 = 0$](https://render.githubusercontent.com/render/math?math=%24G_T%20%3D%20G_5%20%3D%200%24),

![\begin{align*} G_5 &= 0 \\ G_4 &= R_5 + \gamma G_5 = 2 + (0.5)(0) = 2 \\ G_3 &= R_4 + \gamma G_4 = 3 + (0.5)(2) = 4 \\ G_2 &= R_3 + \gamma G_3 = 6 + (0.5)(4) = 8 \\ G_1 &= R_2 + \gamma G_2 = 2 + (0.5)(8) = 6 \\ G_0 &= R_1 + \gamma G_1 = -1 + (0.5)(6) = 2. \end{align*}](https://render.githubusercontent.com/render/math?math=%5Cbegin%7Balign*%7D%20G_5%20%26%3D%200%20%5C%5C%20G_4%20%26%3D%20R_5%20%2B%20%5Cgamma%20G_5%20%3D%202%20%2B%20(0.5)(0)%20%3D%202%20%5C%5C%20G_3%20%26%3D%20R_4%20%2B%20%5Cgamma%20G_4%20%3D%203%20%2B%20(0.5)(2)%20%3D%204%20%5C%5C%20G_2%20%26%3D%20R_3%20%2B%20%5Cgamma%20G_3%20%3D%206%20%2B%20(0.5)(4)%20%3D%208%20%5C%5C%20G_1%20%26%3D%20R_2%20%2B%20%5Cgamma%20G_2%20%3D%202%20%2B%20(0.5)(8)%20%3D%206%20%5C%5C%20G_0%20%26%3D%20R_1%20%2B%20%5Cgamma%20G_1%20%3D%20-1%20%2B%20(0.5)(6)%20%3D%202.%20%5Cend%7Balign*%7D)


Exercise 3.9: Suppose ![$\gamma = 0.9$](https://render.githubusercontent.com/render/math?math=%24%5Cgamma%20%3D%200.9%24) and the reward sequence is ![$R_1 = 2$](https://render.githubusercontent.com/render/math?math=%24R_1%20%3D%202%24) followed by an infinite sequence of 7s. What are ![$G_1$](https://render.githubusercontent.com/render/math?math=%24G_1%24) and ![$G_0$](https://render.githubusercontent.com/render/math?math=%24G_0%24)?

Using the definition of the discounted return (![$G_t = \Sigma_{k=0}^{\infty} \gamma^k R_{t+k+1}$](https://render.githubusercontent.com/render/math?math=%24G_t%20%3D%20%5CSigma_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20R_%7Bt%2Bk%2B1%7D%24)), ![$G_1 = \Sigma_{k=0}^{\infty} \gamma^k R_{k+2} = 7 \left( \Sigma_{k=0}^{\infty} \gamma^k \right) = 7 \left( \frac{1}{1-\gamma} \right) = 7\left( \frac{1}{1-0.9} \right) = 7(10) = 70$](https://render.githubusercontent.com/render/math?math=%24G_1%20%3D%20%5CSigma_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20R_%7Bk%2B2%7D%20%3D%207%20%5Cleft(%20%5CSigma_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20%5Cright)%20%3D%207%20%5Cleft(%20%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%20%5Cright)%20%3D%207%5Cleft(%20%5Cfrac%7B1%7D%7B1-0.9%7D%20%5Cright)%20%3D%207(10)%20%3D%2070%24).

Using Equation 3.9 from the text, ![$G_0 = R_1 + \gamma G_1 = 2 + (0.9)(70) = 65$](https://render.githubusercontent.com/render/math?math=%24G_0%20%3D%20R_1%20%2B%20%5Cgamma%20G_1%20%3D%202%20%2B%20(0.9)(70)%20%3D%2065%24).

Exercise 3.10: Prove the second equality in (3.10).

Define ![$S \equiv \Sigma_{k=0}^{\infty} \gamma^k = 1 + \gamma + \gamma^2 + \cdots$](https://render.githubusercontent.com/render/math?math=%24S%20%5Cequiv%20%5CSigma_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20%3D%201%20%2B%20%5Cgamma%20%2B%20%5Cgamma%5E2%20%2B%20%5Ccdots%24).  Note that ![$\gamma S = \gamma + \gamma^2 + \gamma^3 + \cdots = S - 1$](https://render.githubusercontent.com/render/math?math=%24%5Cgamma%20S%20%3D%20%5Cgamma%20%2B%20%5Cgamma%5E2%20%2B%20%5Cgamma%5E3%20%2B%20%5Ccdots%20%3D%20S%20-%201%24).  Thus, ![$S - \gamma S = (1 - \gamma) S = 1$](https://render.githubusercontent.com/render/math?math=%24S%20-%20%5Cgamma%20S%20%3D%20(1%20-%20%5Cgamma)%20S%20%3D%201%24), and so ![$S = \frac{1}{1 - \gamma}$](https://render.githubusercontent.com/render/math?math=%24S%20%3D%20%5Cfrac%7B1%7D%7B1%20-%20%5Cgamma%7D%24).

Exercise 3.14: The Bellman equation (3.14) must hold for each state for the value function ![$v_{\pi}$](https://render.githubusercontent.com/render/math?math=%24v_%7B%5Cpi%7D%24) shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, -0.4, and +0.7. (These numbers are accurate only to one decimal place.)

Figure 3.2 was generated under the conditions that all four actions are equally probable, that all actions that stay on the grid and are not out of states A and B receive reward 0, and with a discount rate of ![$\gamma = 0.9$](https://render.githubusercontent.com/render/math?math=%24%5Cgamma%20%3D%200.9%24).  Labeling the neighboring states ![$s'$](https://render.githubusercontent.com/render/math?math=%24s'%24) as (up, right, down, left), there are four distinct ![$(a, s', r)$](https://render.githubusercontent.com/render/math?math=%24(a%2C%20s'%2C%20r)%24) triples which contribute to the sum in the Bellman equation: (north, up, 0), (east, right, 0), (south, down, 0), and (west, left, 0).  Since each of the four actions is equally probable,

![$\sum_{\{a,s',r\}} \pi(a \mid s) p(s',r \mid s,a) \left\[ r + \gamma v_{\pi}(s') \right\] = \frac{1}{4} \gamma \sum_{s'} v_{\pi}(s') = \frac{0.9}{4} \left(2.3+0.4-0.4+0.7\right) \approx 0.7, $](https://render.githubusercontent.com/render/math?math=%24%5Csum_%7B%5C%7Ba%2Cs'%2Cr%5C%7D%7D%20%5Cpi(a%20%5Cmid%20s)%20p(s'%2Cr%20%5Cmid%20s%2Ca)%20%5Cleft%5B%20r%20%2B%20%5Cgamma%20v_%7B%5Cpi%7D(s')%20%5Cright%5D%20%3D%20%5Cfrac%7B1%7D%7B4%7D%20%5Cgamma%20%5Csum_%7Bs'%7D%20v_%7B%5Cpi%7D(s')%20%3D%20%5Cfrac%7B0.9%7D%7B4%7D%20%5Cleft(2.3%2B0.4-0.4%2B0.7%5Cright)%20%5Capprox%200.7%2C%20%24)

which equals the value of the center state in Figure 3.2.

Exercise 3.15: In the gridworld example, rewards are positive for goals, negative for running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant ![$c$](https://render.githubusercontent.com/render/math?math=%24c%24) to all the rewards adds a constant, ![$v_c$](https://render.githubusercontent.com/render/math?math=%24v_c%24), to the values of all states, and thus does not affect the relative values of any states under any policies. What is ![$v_c$](https://render.githubusercontent.com/render/math?math=%24v_c%24) in terms of ![$c$](https://render.githubusercontent.com/render/math?math=%24c%24) and ![$\gamma$](https://render.githubusercontent.com/render/math?math=%24%5Cgamma%24)?

Only the intervals between rewards are important.

From Equation (3.8), if a constant ![$c$](https://render.githubusercontent.com/render/math?math=%24c%24) is added to all rewards, then the discounted return is shifted according to ![$G_t = \Sigma_{k=0}^{\infty} \gamma^k R_{t+k+1} \rightarrow \Sigma_{k=0}^{\infty} \gamma^k \left( R_{t+k+1} + c \right) = G_t + c \Sigma_{k=0}^{\infty} \gamma^k = G_t + \frac{c}{1-\gamma}$](https://render.githubusercontent.com/render/math?math=%24G_t%20%3D%20%5CSigma_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20R_%7Bt%2Bk%2B1%7D%20%5Crightarrow%20%5CSigma_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20%5Cleft(%20R_%7Bt%2Bk%2B1%7D%20%2B%20c%20%5Cright)%20%3D%20G_t%20%2B%20c%20%5CSigma_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%20%5Cgamma%5Ek%20%3D%20G_t%20%2B%20%5Cfrac%7Bc%7D%7B1-%5Cgamma%7D%24).  Thus, from Equation (3.12), the value function correspondingly shifts according to ![$v_{\pi}(s) = \mathbb{E}_{\pi} \left\[ G_t \mid S_t = s \right\] \rightarrow \mathbb{E}_{\pi} \left\[ G_t \mid S_t = s \right\] + \frac{c}{1-\gamma}$](https://render.githubusercontent.com/render/math?math=%24v_%7B%5Cpi%7D(s)%20%3D%20%5Cmathbb%7BE%7D_%7B%5Cpi%7D%20%5Cleft%5B%20G_t%20%5Cmid%20S_t%20%3D%20s%20%5Cright%5D%20%5Crightarrow%20%5Cmathbb%7BE%7D_%7B%5Cpi%7D%20%5Cleft%5B%20G_t%20%5Cmid%20S_t%20%3D%20s%20%5Cright%5D%20%2B%20%5Cfrac%7Bc%7D%7B1-%5Cgamma%7D%24).  Therefore, the values of all states shift by the constant ![$v_c \equiv \frac{c}{1-\gamma}$](https://render.githubusercontent.com/render/math?math=%24v_c%20%5Cequiv%20%5Cfrac%7Bc%7D%7B1-%5Cgamma%7D%24).

Exercise 3.16: Now consider adding a constant ![$c$](https://render.githubusercontent.com/render/math?math=%24c%24) to all the rewards in an episodic task, such as maze running. Would this have any effect, or would it leave the task unchanged as in the continuing task above? Why or why not? Give an example.

For an episodic task with ![$T$](https://render.githubusercontent.com/render/math?math=%24T%24) steps per episode, the discounted return is given by a modified version of Equation (3.8): ![$G_t = \Sigma_{k=0}^{T} \gamma^k R_{t+k+1}$](https://render.githubusercontent.com/render/math?math=%24G_t%20%3D%20%5CSigma_%7Bk%3D0%7D%5E%7BT%7D%20%5Cgamma%5Ek%20R_%7Bt%2Bk%2B1%7D%24).  Now, if a constant ![$c$](https://render.githubusercontent.com/render/math?math=%24c%24) is added to all rewards, then the discounted return is shifted according to ![$G_t = \Sigma_{k=0}^{T} \gamma^k R_{t+k+1} \rightarrow \Sigma_{k=0}^{T} \gamma^k \left( R_{t+k+1} + c \right) = G_t + c \Sigma_{k=0}^{T} \gamma^k = G_t + c \left(\frac{1-\gamma^{T+1}}{1-\gamma}\right)$](https://render.githubusercontent.com/render/math?math=%24G_t%20%3D%20%5CSigma_%7Bk%3D0%7D%5E%7BT%7D%20%5Cgamma%5Ek%20R_%7Bt%2Bk%2B1%7D%20%5Crightarrow%20%5CSigma_%7Bk%3D0%7D%5E%7BT%7D%20%5Cgamma%5Ek%20%5Cleft(%20R_%7Bt%2Bk%2B1%7D%20%2B%20c%20%5Cright)%20%3D%20G_t%20%2B%20c%20%5CSigma_%7Bk%3D0%7D%5E%7BT%7D%20%5Cgamma%5Ek%20%3D%20G_t%20%2B%20c%20%5Cleft(%5Cfrac%7B1-%5Cgamma%5E%7BT%2B1%7D%7D%7B1-%5Cgamma%7D%5Cright)%24).  Notably, the values are now shifted by a non-constant term that depends on ![$T$](https://render.githubusercontent.com/render/math?math=%24T%24), the number of steps per episode.  Depending on the sign of the constant ![$c$](https://render.githubusercontent.com/render/math?math=%24c%24), this ![$T$](https://render.githubusercontent.com/render/math?math=%24T%24)-dependent term can result in optimal policies that prefer shorter or longer steps per episode.
